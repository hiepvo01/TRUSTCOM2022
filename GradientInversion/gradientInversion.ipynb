{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from aijack.attack import GradientInversion_Attack\n",
    "from aijack.utils import NumpyDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, channel=3, hideen=588, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        act = nn.Sigmoid\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(channel, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
    "            nn.BatchNorm2d(12),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
    "            nn.BatchNorm2d(12),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=1),\n",
    "            nn.BatchNorm2d(12),\n",
    "            act(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hideen, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# batch_size = 3\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "#                                         download=True, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "#                                           shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "#                                        download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "# images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "# show images\n",
    "# imshow(torchvision.utils.make_grid(images.cpu()))\n",
    "# print labels\n",
    "# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a7be982fd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = torch.load('generated_images.pth')\n",
    "img = images[0][0]\n",
    "print(img.shape)\n",
    "plt.imshow(torchvision.utils.make_grid(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_out(attacker, received_gradients, batch_size=0):\n",
    "    num_seeds=3\n",
    "    fig = plt.figure()\n",
    "    fig.set_figheight(7)\n",
    "    fig.set_figwidth(7)\n",
    "    for s in tqdm(range(num_seeds)):\n",
    "        attacker.reset_seed(s)\n",
    "        if batch_size > 0:\n",
    "            result = attacker.attack(received_gradients, batch_size=batch_size)\n",
    "        else:\n",
    "            result = attacker.attack(received_gradients)\n",
    "        ax1 = fig.add_subplot(3, num_seeds, s+1)\n",
    "        test_img = torch.from_numpy((result[0].cpu().detach().numpy()[0])) / 2 + 0.5\n",
    "        img1 = test_img.swapaxes(0,1)\n",
    "        img1 = img1.swapaxes(1,2)\n",
    "        ax1.imshow(torchvision.utils.make_grid(img1))\n",
    "        ax1.set_title(torch.argmax(result[1]).item())\n",
    "        ax1.axis(\"off\")\n",
    "        ax2 = fig.add_subplot(3, num_seeds, num_seeds+s+1)\n",
    "        ax2.imshow(result[0].cpu().detach().numpy()[0][0], cmap=\"gray\")\n",
    "        ax2.axis(\"off\")\n",
    "        ax3 = fig.add_subplot(3, num_seeds, num_seeds + num_seeds+s+1)\n",
    "        ax3.imshow(cv2.medianBlur(result[0].cpu().detach().numpy()[0][0], 5), cmap=\"gray\")\n",
    "        ax3.axis(\"off\")\n",
    "    plt.suptitle(\"Result\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLG Attack\n",
    "https://dlg.mit.edu/assets/NeurIPS19_deep_leakage_from_gradients.pdf\n",
    "\n",
    "- distance metric = L2 norm\n",
    "- optimize both of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(images[0].cpu()))\n",
    "print(\"Target\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = LeNet(channel=3, hideen=768, num_classes=10)\n",
    "net.to(device)\n",
    "pred = net(images[:1])\n",
    "loss = criterion(pred, labels[:1])\n",
    "received_gradients = torch.autograd.grad(loss, net.parameters())\n",
    "received_gradients = [cg.detach() for cg in received_gradients]\n",
    "\n",
    "dlg_attacker = GradientInversion_Attack(net, (3, 32, 32), lr=1.0, log_interval=0,\n",
    "                                    num_iteration=1000,\n",
    "                                    distancename=\"l2\", device=device)\n",
    "\n",
    "draw_out(dlg_attacker, received_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_out(dlg_attacker, received_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GS Attack\n",
    "https://arxiv.org/abs/2003.14053\n",
    "- distance metric = cosine similarity\n",
    "- optimize both of images and labels\n",
    "- total-variance regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow(torchvision.utils.make_grid(images[0].cpu()))\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# net = LeNet(channel=3, hideen=768, num_classes=10)\n",
    "# net.to(device)\n",
    "# pred = net(images[:1])\n",
    "# loss = criterion(pred, labels[:1])\n",
    "# received_gradients = torch.autograd.grad(loss, net.parameters())\n",
    "# received_gradients = [cg.detach() for cg in received_gradients]\n",
    "\n",
    "# gs_attacker = GradientInversion_Attack(net, (3, 32, 32), lr=1.0, log_interval=0,\n",
    "#                                     num_iteration=200,\n",
    "#                                     tv_reg_coef=0.01,\n",
    "#                                     distancename=\"cossim\", device=device)\n",
    "\n",
    "# draw_out(gs_attacker, received_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDLG Attack\n",
    "https://arxiv.org/abs/2001.02610\n",
    "- distance metric = L2 norm\n",
    "- optimize only an image & estimate a label from the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(images[0].cpu()))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = LeNet(channel=3, hideen=768, num_classes=10)\n",
    "net.to(device)\n",
    "pred = net(images[:1])\n",
    "loss = criterion(pred, labels[:1])\n",
    "received_gradients = torch.autograd.grad(loss, net.parameters())\n",
    "received_gradients = [cg.detach() for cg in received_gradients]\n",
    "\n",
    "idlg_attacker = GradientInversion_Attack(net, (3, 32, 32), lr=10e2, log_interval=0,\n",
    "                                    optimizer_class=torch.optim.SGD,\n",
    "                                    distancename=\"l2\", optimize_label=False,\n",
    "                                    num_iteration=10000, device=device)\n",
    "\n",
    "draw_out(idlg_attacker, received_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPL Attack\n",
    "https://arxiv.org/abs/2004.10397\n",
    "- distance metric = L2 norm\n",
    "- optimize only images & estimate an label from the gradients\n",
    "- label-matching regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images[0].cpu()))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = LeNet(channel=3, hideen=768, num_classes=10)\n",
    "net.to(device)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "# for epoch in range(10):\n",
    "pred = net(images[:1])\n",
    "loss = criterion(pred, labels[:1])\n",
    "loss.backward(retain_graph=True)\n",
    "received_gradients = torch.autograd.grad(loss, net.parameters())\n",
    "received_gradients = [cg.detach()for cg in received_gradients]\n",
    "\n",
    "cpl_attacker = GradientInversion_Attack(net, (3, 32, 32), lr=10e2, log_interval=0,\n",
    "                                    optimizer_class=torch.optim.SGD,\n",
    "                                    distancename=\"l2\", optimize_label=False,\n",
    "                                    num_iteration=10000,\n",
    "                                    lm_reg_coef=0.01, device=device)\n",
    "\n",
    "draw_out(cpl_attacker, received_gradients)\n",
    "\n",
    "    # optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Atk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# imshow(torchvision.utils.make_grid(images[0:3].cpu()))\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# net = LeNet(channel=3, hideen=768, num_classes=10)\n",
    "# net.to(device)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "# for epoch in range(3):\n",
    "#     pred = net(images[:1])\n",
    "\n",
    "#     loss = criterion(pred, labels[:1])\n",
    "#     loss.backward(retain_graph=True)\n",
    "\n",
    "#     received_gradients = torch.autograd.grad(loss, net.parameters())\n",
    "#     received_gradients = [cg.detach()/(-0.1)  for cg in received_gradients]\n",
    "    \n",
    "#     optimizer.step()\n",
    "    \n",
    "# cpl_attacker = GradientInversion_Attack(net, (3, 32, 32), lr=10e2, log_interval=0,\n",
    "#                                     optimizer_class=torch.optim.SGD,\n",
    "#                                     distancename=\"l2\", optimize_label=False,\n",
    "#                                     num_iteration=10000,\n",
    "#                                     lm_reg_coef=0.01, device=device)\n",
    "\n",
    "# draw_out(cpl_attacker, received_gradients, batch_size=3)\n",
    "    \n",
    "# gradinversion = GradientInversion_Attack(net, (3, 32, 32), num_iteration=1900,\n",
    "#                                         lr=1e2, log_interval=0,\n",
    "#                                         optimizer_class=torch.optim.SGD,\n",
    "#                                         distancename=\"l2\", optimize_label=False,\n",
    "#                                         bn_reg_layers=[net.body[1], net.body[4], net.body[7]],\n",
    "#                                         group_num = 3,\n",
    "#                                         tv_reg_coef=0.00, l2_reg_coef=0.0001,\n",
    "#                                         bn_reg_coef=0.001, gc_reg_coef=0.001, device=device)\n",
    "\n",
    "# result = gradinversion.group_attack(received_gradients, batch_size=batch_size)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# for bid in range(batch_size):\n",
    "#     ax1 = fig.add_subplot(2, batch_size, bid+1)\n",
    "#     test_img = torch.from_numpy(((sum(result[0]) / len(result[0])).cpu().detach().numpy()[bid]))\n",
    "#     img1 = test_img.swapaxes(0,1)\n",
    "#     img1 = img1.swapaxes(1,2)\n",
    "#     ax1.imshow(torchvision.utils.make_grid(img1))\n",
    "#     ax1.set_title(result[1][0][bid].item())\n",
    "#     ax1.axis(\"off\")\n",
    "#     ax2 = fig.add_subplot(2, batch_size, batch_size+bid+1)\n",
    "#     ax2.imshow((sum(result[0]) / len(result[0])).cpu().detach().numpy()[bid][0], cmap=\"gray\")\n",
    "#     ax2.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradInversion\n",
    "https://arxiv.org/abs/2104.07586\n",
    "- distance metric = L2 norm\n",
    "- optimize only images & estimate labels from the gradients\n",
    "- total-variance, l2, bn, and group-consistency regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in torch.load('../data/cifar10.pth')]\n",
    "for i in range(len(train_loader)):\n",
    "    dataiter = iter(train_loader[i])\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "\n",
    "    imshow(torchvision.utils.make_grid(images.cpu()))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    net = LeNet(channel=3, hideen=768, num_classes=10)\n",
    "    net.to(device)\n",
    "    pred = net(images[:batch_size])\n",
    "    loss = criterion(pred, labels[:batch_size])\n",
    "    received_gradients = torch.autograd.grad(loss, net.parameters())\n",
    "    received_gradients = [cg.detach() for cg in received_gradients]\n",
    "\n",
    "    gradinversion = GradientInversion_Attack(net, (3, 32, 32), num_iteration=900,\n",
    "                                        lr=1e2, log_interval=0,\n",
    "                                        optimizer_class=torch.optim.SGD,\n",
    "                                        distancename=\"l2\", optimize_label=False,\n",
    "                                        bn_reg_layers=[net.body[1], net.body[4], net.body[7]],\n",
    "                                        group_num = 3,\n",
    "                                        tv_reg_coef=0.00, l2_reg_coef=0.0001,\n",
    "                                        bn_reg_coef=0.001, gc_reg_coef=0.001, device=device)\n",
    "\n",
    "    result = gradinversion.group_attack(received_gradients, batch_size=batch_size)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    for bid in range(batch_size):\n",
    "        ax1 = fig.add_subplot(2, batch_size, bid+1)\n",
    "        test_img = torch.from_numpy(((sum(result[0]) / len(result[0])).cpu().detach().numpy()[bid]))\n",
    "        img1 = test_img.swapaxes(0,1)\n",
    "        img1 = img1.swapaxes(1,2)\n",
    "        ax1.imshow(torchvision.utils.make_grid(img1))\n",
    "        ax1.set_title(result[1][0][bid].item())\n",
    "        ax1.axis(\"off\")\n",
    "        # ax2 = fig.add_subplot(2, batch_size, batch_size+bid+1)\n",
    "        # ax2.imshow((sum(result[0]) / len(result[0])).cpu().detach().numpy()[bid][0], cmap=\"gray\")\n",
    "        # ax2.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Result of GradInversion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "968934d88acb7fa4003ed11a52074e68ba95c397f9757dc45b04b92654f10ea7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
