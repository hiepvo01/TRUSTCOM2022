{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset \n",
    "from aijack.attack import GradientInversion_Attack\n",
    "from matplotlib import pyplot as plt  \n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 10\n",
    "num_selected = 10\n",
    "num_rounds = 5\n",
    "epochs = 5\n",
    "batch_size = 3\n",
    "client_victim = 1\n",
    "data = \"MNIST\"\n",
    "chosen_model = ''\n",
    "if data == \"CIFAR10\":\n",
    "    chosen_model = 'test'\n",
    "    channels = 3\n",
    "    norm = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    hideen=768\n",
    "    img_shape = (channels, 32, 32)\n",
    "else:\n",
    "    chosen_model = 'test'\n",
    "    channels = 1\n",
    "    norm = transforms.Normalize((0.5), (0.5))\n",
    "    hideen=588\n",
    "    img_shape = (channels, 28, 28)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "##### Creating desired data distribution among clients  #####\n",
    "#############################################################\n",
    "\n",
    "# Image augmentation \n",
    "transform_train = transforms.Compose([\n",
    "    # transforms.RandomCrop(32, padding=4),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # norm,\n",
    "])\n",
    "\n",
    "if data == \"CIFAR10\":\n",
    "    # Loading CIFAR10 using torchvision.datasets\n",
    "    traindata = datasets.CIFAR10('./data', train=True, download=True,\n",
    "                        transform= transform_train)\n",
    "else:\n",
    "    # Loading CIFAR10 using torchvision.datasets\n",
    "    traindata = datasets.FashionMNIST('./data', train=True, download=True,\n",
    "                        transform= transform_train)\n",
    "\n",
    "# Dividing the training data into num_clients, with each client having equal number of images\n",
    "traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
    "\n",
    "print(len(traindata))\n",
    "\n",
    "traindata_split = torch.utils.data.random_split(traindata, [200 for _ in range(300)])\n",
    "traindata_split = traindata_split[:10]\n",
    "torch.save(traindata_split, '../../data/fashion.pth')\n",
    "\n",
    "# Creating a pytorch loader for a Deep Learning model\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in torch.load('../../data/fashion.pth')]\n",
    "\n",
    "# Normalizing the test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    norm,\n",
    "])\n",
    "\n",
    "if data == \"CIFAR10\":\n",
    "    # Loading the test iamges and thus converting them into a test_loader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            datasets.CIFAR10('./data', train=False, transform=transform_test\n",
    "            ), batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "    # Loading the test iamges and thus converting them into a test_loader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            datasets.FashionMNIST('./data', train=False, transform=transform_test\n",
    "            ), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAACWCAYAAADHc9MUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAARIklEQVR4nO3dfWyVVZ4H8O/PKtkpRLEo2AJaN1SQTFKRyjqZRRsZIzJERFHHOEAMWjVjhtFBeVOxslYUI2oChAbdJYLTVUCtwO6KCIrJrlJl5KW8FNnlpdQiruN0rRkEfvvHfXg459j70t7X0/v9JIbze8699zm5vf359HfPeY6oKoiIyD9nZXsARETUNUzgRESeYgInIvIUEzgRkaeYwImIPMUETkTkqaQSuIiMFpE9IrJPRGakalBERBSfdHUeuIgUANgL4HoAhwFsAXCnqjambnhERBTN2Uk8dwSAfaq6HwBEpA7AOABRE3hhYaH27t07iVMSEeWflpaWY6p6oXs8mQTeH8AhIz4M4B9iPaF3796oqqpK4pRERPmnurr6QEfH0/4lpohUiUiDiDS0t7en+3RERHkjmQTeDGCgEQ8IjllUtVZVK1S1orCwMInTERGRKZkEvgVAmYhcKiI9APwGQH1qhkVERPF0uQauqidE5EEA/wGgAMCrqrozZSMjIqKYkvkSE6q6DsC6FI2FiIg6IakEnkrV1dXZHgJ1YM6cOTH7+XPLPfyZ+Snez60jXEpPROQpJnAiIk8xgRMReYoJnIjIU0zgRESeYgInIvIUEzgRkaeYwImIPMUETkTkKSZwIiJP5cxSeiIi1zXXXGPFRUVFVnzrrbeG7f79+1t9lZWVUV+3vt6+ceo999xjxceOHevMMLOGV+BERJ5iAici8hQTOBGRp1gDJ0rAkCFDwvbbb78d87Fz584N2ytWrEjXkLqtZ555JmxPmzbN6isoKEjJOW666SYr3rBhgxWXl5en5DzpxitwIiJPMYETEXmKCZyIyFOsgeeYiy++OGxfcMEFVt/s2bOt+Oabbw7b33zzjdU3fPhwKz506FCKRpifdu48s1+3qsZ87LJly8I2a+Cd19jYGLZPnjxp9cWqgbe3t1txc3OzFbe1tUV93auuusqKa2pqwvZjjz1m9Z06dSrqGDKNV+BERJ5iAici8hRLKFl2yy23WPHixYvDdp8+faw+EbFi809597Fu+YUllOQ0NTWF7UGDBmVxJN3fa6+9FrbdctW5555rxVu2bAnb3333ndW3d+/eqOdwl+S7S+dnzJgRtufPn2/1ffvtt1FfN9N4BU5E5CkmcCIiTzGBExF5ijXwDDCXYQPA6tWrw/bgwYOtPrPO7db/3Dp2bW1th68JALt37+7aYKlD06dPD9vz5s2z+srKyjI9nLyxfPnytLxuaWlpWl4303gFTkTkqbgJXEReFZGjIrLDOFYkIutFpCn49/z0DpOIiFyJXIH/C4DRzrEZADaoahmADUFMREQZFLcGrqofiUipc3gcgMqgvQzAJgDTkcfM+dwzZ860+twaeGFhYdh269ybN28O22+99ZbV59YD3eXzlD5r1qwJ2+72W5dddpkVL1iwICNjotjOO+88KzZvTevelsK1bdu2sP3999+ndmAp1NUaeD9VbQnaXwHol6LxEBFRgpL+ElMjl5BR7+4jIlUi0iAiDe7NZoiIqOu6Oo2wVUSKVbVFRIoBHI32QFWtBVALACUlJbFv45bD3KXpS5YssWLzzoCxlrwDwEsvvRS23el/H3/8cTLDpDS58sorw/aNN95o9bk/35aWFlDmjRw50oqff/55K3bvOBjLxIkTw/bx48eTG1gadfUKvB7A5KA9GcA7qRkOERElKpFphH8C8J8ABovIYRGZAmAegOtFpAnAr4KYiIgyKJFZKHdG6RqV4rEQEVEncCl9DGZN7YUXXrD6zJooEHuXFrfPrHOz5u2HeDvRm+rq6tI3kDznTg00b3Hw4IMPWn29evWK+jrujjyPP/64FW/fvr2rQ8woLqUnIvIUEzgRkaeYwImIPJXXNXBzB3jgp0ugx48fH7bdOrY71zvRPgBYuXJl1Nc1l8ePHm3fgubzzz+P+bqUOvfee68VX3TRRWHb/ZmtXbvWijkPPHVGjBhhxYsWLbJi97uoWFpbW8P25MmTrb733nuvC6PLPl6BExF5igmciMhTeV1CMXe0Bn66s7v5p3KsaYKAvSTe3eHaZfa7dy40x+BOXaysrIz5utR15eXlVuwuwz7rrDPXOqdOnbL6fvjhByt2+6lzzN+BxYsXW33Dhg1L+HXc55q/T19++WUXR5dbeAVOROQpJnAiIk8xgRMReSqva+AHDx60YnOnHMDe2d3dHaempiYlY3CnCg4fPjxsx6ulU+pce+21VtyzZ08rNuvabW1tVt/evXvTN7A8MGbMGCs2v0/q0aNHzOceOXIkbE+aNMnq++CDD1IwutzGK3AiIk8xgRMReSqvSyjuDh2DBw+24j179qTlvObuPu5OP+Z0RXOXH0o9cyPqp556KuZjzamCd999t9XnltcoPvN3zZ0uG6ts8uGHH1qxuWJ23759KRqdP3gFTkTkKSZwIiJPMYETEXkqr2vgrnTVvF3mXRDdOyKadzLkNML0uuOOO8J2rN1bAGD9+vVhmzXvzuvbt68Vv/vuu2F70KBBUZ9nPg4ApkyZYsX5/jvCK3AiIk8xgRMReYoJnIjIU6yBZ4E5/zjWbWrNJcWUvPvvv9+KJ0yYkPBz586dm+rhdGvu7vHr1q2zYrPu7e5gtWnTprD96KOPWn35XvN28QqciMhTTOBERJ5iCSULzBKK++ejuWTb141Wc8nYsWPD9sKFC60+s3xlbiYNAHV1dVbMDaU7Z/r06VYca/Ph5uZmK8735fGdwStwIiJPxU3gIjJQRDaKSKOI7BSRqcHxIhFZLyJNwb/np3+4RER0WiJX4CcA/FFVhwK4GsDvRGQogBkANqhqGYANQUxERBkStwauqi0AWoJ2m4jsAtAfwDgAlcHDlgHYBGB6By+RdiNHjgzb48ePt/oefvjhTA/nJ2bPnm3F5q003WmEu3btCttcst15o0aNsuKXX345oedt3LjRiqdOnZqyMeWj2267LeHHut8DzZkzJ+pjhwwZYsXmDlYHDhyw+tzbI7z55pth252i+/777yc22BzTqRq4iJQCGAbgEwD9guQOAF8B6JfaoRERUSwJJ3AR6QVgFYA/qOpfzT6NXEZ2uCJFRKpEpEFEGtrb25MaLBERnZFQAheRcxBJ3itU9fTfHq0iUhz0FwM42tFzVbVWVStUtcLdNJiIiLoubg1cIgWqVwDsUlVz76N6AJMBzAv+fSctI0zADTfcELbd2qW767sZu7vSp8qqVaus2N0azaz5bd682eqrrKxMy5i6q+LiYitesGCBFbu36zW1tLSE7XhbqlF85lzv0tLShJ/n/gzvuuuuLp3/kksuidlv3kqhqqrK6nO3dXv22WfDtrtGIJckspDnlwAmAtguIn8Ojs1CJHG/ISJTABwAcHtaRkhERB1KZBbKxwAkSveoKMeJiCjNusVS+qeffjpsu+WKyy+/3IpnzpwZth944IEun9PdTX7WrFlRx+BOFTTLJg899FCXx5CvzDvdudP/ysrKoj7PXbJtTjlsampK0ejy14kTJ8L2yZMnrb6CgoK0nLOtrS1st7a2Wn3u711RUVHY7tOnj9U3bdo0K540aVLYnj9/vtXnTk398ccfOzHi1OJSeiIiTzGBExF5igmciMhT3aIGbt6C1a1PLVmyxIrN6UOvv/661edO6TO5Ne+PPvrIis3l8eZ4AHt5PGBPZ9q9e3fUc1LHli9fHrbdHc1j7XDkLrNn3Tu1tm3bFrbr6+utvpKSEituaGgI2+5SevNnuGPHDqvviy++sGJzit/+/ftjjs8cw5gxY6w+dxph3759w7ZbAy8vL7fiRx55JGy7dfh04xU4EZGnmMCJiDzFBE5E5KluUQM3ucvY3aX1Zq3a3LoJ+OmccbN27T7WfB3ArtvV1NRYfW5MyTHrl27N+/Dhw1a8aNGisM2ad+bcfnvuLcw+cuRI2F66dKnVt3btWis214iYNW4AGDdunBU/+eSTKRph5/EKnIjIU0zgRESe6nYlFPfOYeaSWAD49NNPw/bEiROtPvcuaOb0JvdPdXeqoLnDB0sm6WX++TtlyhSrzyyZAMBzzz2XkTGR38w7UwLAE0880WE71/AKnIjIU0zgRESeYgInIvJUt6uBuxobG624oqIibLu7xbu3gTWX1rvL4V988UUr3rNnTxKjpM647777OmwT5RtegRMReYoJnIjIU0zgRESe6vY1cHe+9tatW8P2hAkTMj0cIqKU4RU4EZGnmMCJiDzFBE5E5CkmcCIiTzGBExF5igmciMhTTOBERJ6Km8BF5O9E5FMR+UJEdopIdXD8UhH5RET2ici/ikiP9A+XiIhOS+QK/G8ArlPVcgBXABgtIlcDeBbAAlUdBOBbAFOivwQREaVa3ASuEf8XhOcE/ymA6wCsDI4vA3BzOgZIREQdE3ersA4fJFIA4DMAgwAsBDAfwH8FV98QkYEA/k1Vfx7rdUpKSrSqqirpQRMR5ZPq6urPVLXCPZ7Ql5iqelJVrwAwAMAIAEMSPbGIVIlIg4g0tLe3J/o0IiKKo1OzUFT1LwA2AvgFgN4icvpmWAMANEd5Tq2qVqhqRWFhYTJjJSIiQyKzUC4Ukd5B+2cArgewC5FEfvp2fpMBvJOmMRIRUQcSuZ1sMYBlQR38LABvqOoaEWkEUCci/wRgK4BX0jhOIiJyxE3gqroNwLAOju9HpB5ORERZwJWYRESeSmgaYcpOJvI1gAMALgBwLGMn9g/fn/j4HsXH9yg+X96jS1T1QvdgRhN4eFKRho7mNFIE35/4+B7Fx/coPt/fI5ZQiIg8xQROROSpbCXw2iyd1xd8f+LjexQf36P4vH6PslIDJyKi5LGEQkTkqYwmcBEZLSJ7gk0gZmTy3LlKRAaKyEYRaQw2zJgaHC8SkfUi0hT8e362x5pNIlIgIltFZE0Qc0MRg4j0FpGVIrJbRHaJyC/4GbKJyEPB79gOEflTsFmN15+jjCXwYCn+QgA3AhgK4E4RGZqp8+ewEwD+qKpDAVwN4HfB+zIDwAZVLQOwIYjz2VRE7sFzGjcUsb0E4N9VdQiAckTeK36GAiLSH8DvAVQEt70uAPAbeP45yuQV+AgA+1R1v6oeB1AHYFwGz5+TVLVFVT8P2m2I/OL1R+S9WRY8LK83zBCRAQB+DWBpEAu4oUhIRM4DcA2C+xGp6vHgzqH8DNnOBvCz4C6qhQBa4PnnKJMJvD+AQ0Z8ODhGAREpReS+M58A6KeqLUHXVwD6ZWtcOeBFAI8COBXEfQD8RVVPBHG+f5YuBfA1gH8OykxLRaQn+BkKqWozgOcBHEQkcX+HyCY1Xn+O+CVmjhCRXgBWAfiDqv7V7NPIVKG8nC4kImMBHFXVz7I9lhx2NoArASxW1WEAvodTLsnnzxAABPX/cYj8z64EQE8Ao7M6qBTIZAJvBjDQiKNuApFvROQcRJL3ClVdHRxuFZHioL8YwNFsjS/LfgngJhH5H0TKbtchUu9NaEORPHEYwGFV/SSIVyKS0PkZOuNXAP5bVb9W1R8BrEbks+X15yiTCXwLgLLgW98eiHyBUJ/B8+ekoJ77CoBdqvqC0VWPyEYZQB5vmKGqM1V1gKqWIvKZ+UBV7wI3FAmp6lcADonI4ODQKACN4GfIdBDA1SJSGPzOnX6PvP4cZfpuhGMQqWcWAHhVVZ/O2MlzlIj8I4DNALbjTI13FiJ18DcAXIzIHRxvV9X/zcogc4SIVAKYpqpjReTvEbkiL0JkQ5Hfqurfsji8rBKRKxD5krcHgP0A7kawAQv4GQIAiEg1gDsQmfm1FcA9iNS8vf0ccSUmEZGn+CUmEZGnmMCJiDzFBE5E5CkmcCIiTzGBExF5igmciMhTTOBERJ5iAici8tT/A19gGfT2v2rCAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog   car   bird \n"
     ]
    }
   ],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader[0])\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images[0].shape)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "##### Neural Network model #####\n",
    "#################################\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, channels=channels, hideen=hideen, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, kernel_size=(3,3), stride=(2,2), padding=1),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3,3), padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3,3), stride=(2,2), padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(128, 256, kernel_size=(3,3), padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(12544, num_classes)\n",
    "            # nn.Linear(hideen, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def client_update(client_model, optimizer, train_loader, epoch=5):\n",
    "    \"\"\"\n",
    "    This function updates/trains client model on client data\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = client_model(data)\n",
    "            # loss = F.nll_loss(output, target)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward(retain_graph=True)\n",
    "            \n",
    "            received_gradients = torch.autograd.grad(loss, client_model.parameters())\n",
    "            received_gradients = [cg.detach() for cg in received_gradients]            \n",
    "                \n",
    "            optimizer.step()\n",
    "    return loss, received_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_aggregate(global_model, client_models):\n",
    "    \"\"\"\n",
    "    This function has aggregation method 'mean'\n",
    "    \"\"\"\n",
    "    ### This will take simple mean of the weights of models ###\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(global_model, test_loader):\n",
    "    \"\"\"This function test the global model on test data and returns test loss and test accuracy \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            output = global_model(data)\n",
    "            # test_loss += F.nll_loss(output, target).item() # sum up batch loss\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = correct / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#### Initializing models and optimizer  ####\n",
    "############################################\n",
    "\n",
    "#### global model ##########\n",
    "global_model =  VGG().cuda()\n",
    "\n",
    "############## client models ##############\n",
    "client_models = [ VGG().cuda() for _ in range(num_selected)]\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict()) ### initial synchronizing with global model \n",
    "\n",
    "############### optimizers ################\n",
    "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th round\n",
      "average train loss 4.47 | test loss 0.747 | test acc: 0.191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th round\n",
      "average train loss 0.35 | test loss 0.22 | test acc: 0.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-th round\n",
      "average train loss 0.653 | test loss 0.182 | test acc: 0.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-th round\n",
      "average train loss 0.338 | test loss 0.161 | test acc: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-th round\n",
      "average train loss 0.026 | test loss 0.145 | test acc: 0.887\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "###### List containing info about learning #########\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "# Runnining FL\n",
    "\n",
    "victim_count = 0 \n",
    "for r in range(num_rounds):\n",
    "    # select random clients\n",
    "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
    "    # client update\n",
    "    loss = 0\n",
    "    \n",
    "    for i in tqdm(range(num_selected)):\n",
    "        client_loss, received_gradients = client_update(client_models[i], opt[i], train_loader[client_idx[i]], epoch=5)\n",
    "        loss += client_loss.item()\n",
    "        \n",
    "    losses_train.append(loss)\n",
    "    # server aggregate\n",
    "    server_aggregate(global_model, client_models)\n",
    "    \n",
    "    test_loss, acc = test(global_model, test_loader)\n",
    "    losses_test.append(test_loss)\n",
    "    acc_test.append(acc)\n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 12544])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0166, -0.0243,  0.0032,  ...,  0.0147, -0.0351, -0.0197],\n",
       "        [-0.0245,  0.0414,  0.0121,  ..., -0.0363,  0.0114,  0.0002],\n",
       "        [ 0.0043,  0.0235, -0.0317,  ...,  0.0393,  0.0180,  0.0314],\n",
       "        ...,\n",
       "        [-0.0178, -0.0539, -0.0018,  ..., -0.0804, -0.0261,  0.0307],\n",
       "        [ 0.0257,  0.0370,  0.0403,  ..., -0.0171,  0.0464, -0.0477],\n",
       "        [ 0.0011, -0.0084, -0.0584,  ..., -0.0084, -0.0399,  0.0015]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(global_model.fc[0].weight.shape)\n",
    "global_model.fc[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body.0.weight': (torch.float32, torch.Size([32, 1, 3, 3])),\n",
       " 'body.0.bias': (torch.float32, torch.Size([32])),\n",
       " 'body.3.weight': (torch.float32, torch.Size([64, 32, 3, 3])),\n",
       " 'body.3.bias': (torch.float32, torch.Size([64])),\n",
       " 'body.4.weight': (torch.float32, torch.Size([64])),\n",
       " 'body.4.bias': (torch.float32, torch.Size([64])),\n",
       " 'body.4.running_mean': (torch.float32, torch.Size([64])),\n",
       " 'body.4.running_var': (torch.float32, torch.Size([64])),\n",
       " 'body.4.num_batches_tracked': (torch.int64, torch.Size([])),\n",
       " 'body.7.weight': (torch.float32, torch.Size([128, 64, 3, 3])),\n",
       " 'body.7.bias': (torch.float32, torch.Size([128])),\n",
       " 'body.8.weight': (torch.float32, torch.Size([128])),\n",
       " 'body.8.bias': (torch.float32, torch.Size([128])),\n",
       " 'body.8.running_mean': (torch.float32, torch.Size([128])),\n",
       " 'body.8.running_var': (torch.float32, torch.Size([128])),\n",
       " 'body.8.num_batches_tracked': (torch.int64, torch.Size([])),\n",
       " 'body.11.weight': (torch.float32, torch.Size([256, 128, 3, 3])),\n",
       " 'body.11.bias': (torch.float32, torch.Size([256])),\n",
       " 'body.12.weight': (torch.float32, torch.Size([256])),\n",
       " 'body.12.bias': (torch.float32, torch.Size([256])),\n",
       " 'body.12.running_mean': (torch.float32, torch.Size([256])),\n",
       " 'body.12.running_var': (torch.float32, torch.Size([256])),\n",
       " 'body.12.num_batches_tracked': (torch.int64, torch.Size([])),\n",
       " 'fc.0.weight': (torch.float32, torch.Size([10, 12544])),\n",
       " 'fc.0.bias': (torch.float32, torch.Size([10]))}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: (v.dtype, v.shape) for k, v in global_model.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 3, 3)\n",
      "(64, 32, 3, 3)\n",
      "(128, 64, 3, 3)\n",
      "(256, 128, 3, 3)\n",
      "(12544, 10)\n"
     ]
    }
   ],
   "source": [
    "torch_weights = global_model.body.state_dict()\n",
    "# Reshape weights for Keras model\n",
    "keras_weights = [w.cpu().numpy() for w in torch_weights.values()]\n",
    "\n",
    "for i in [0, 2, 9, 16]:\n",
    "    print(keras_weights[i].shape)\n",
    "    # conv2d layer: Torch (out,in,h,w) Keras (h,w,in,out)\n",
    "    keras_weights[i] = np.moveaxis(keras_weights[i], [0,1], [-1,-2])\n",
    "\n",
    "keras_weights.append(global_model.fc[0].weight.cpu().detach().numpy().T)\n",
    "print(keras_weights[len(keras_weights)-1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(global_model, \"../../pretrained/test_torch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 3, 3)\n",
      "(64, 32, 3, 3)\n",
      "(128, 64, 3, 3)\n",
      "(256, 128, 3, 3)\n",
      "(12544, 10)\n"
     ]
    }
   ],
   "source": [
    "torch_model = torch.load(\"../../pretrained/test_torch.pt\")\n",
    "torch_weights = torch_model.body.state_dict()\n",
    "# Reshape weights for Keras model\n",
    "keras_weights = [w.cpu().numpy() for w in torch_weights.values()]\n",
    "\n",
    "for i in [0, 2, 9, 16]:\n",
    "    print(keras_weights[i].shape)\n",
    "    # conv2d layer: Torch (out,in,h,w) Keras (h,w,in,out)\n",
    "    keras_weights[i] = np.moveaxis(keras_weights[i], [0,1], [-1,-2])\n",
    "keras_weights.append(torch_model.fc[0].weight.cpu().detach().numpy().T)\n",
    "print(keras_weights[len(keras_weights)-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_keras = [v for v in keras_weights if v.shape != ()]\n",
    "len(new_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2]], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_loader[0])\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images[0].shape)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[0]))\n",
    "pred = torch_model(torch.tensor(np.expand_dims(images[0],0)).cuda())\n",
    "pred.argmax(dim=1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "968934d88acb7fa4003ed11a52074e68ba95c397f9757dc45b04b92654f10ea7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
