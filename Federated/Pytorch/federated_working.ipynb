{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset \n",
    "from aijack.attack import GradientInversion_Attack\n",
    "from matplotlib import pyplot as plt  \n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 10\n",
    "num_selected = 10\n",
    "num_rounds = 5\n",
    "epochs = 5\n",
    "batch_size = 1\n",
    "client_victim = 1\n",
    "data = \"CIFAR10\"\n",
    "chosen_model = ''\n",
    "if data == \"CIFAR10\":\n",
    "    chosen_model = 'test'\n",
    "    channels = 3\n",
    "    norm = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    hideen=768\n",
    "    img_shape = (channels, 32, 32)\n",
    "else:\n",
    "    chosen_model = 'test'\n",
    "    channels = 1\n",
    "    norm = transforms.Normalize((0.5), (0.5))\n",
    "    hideen=588\n",
    "    img_shape = (channels, 28, 28)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "##### Creating desired data distribution among clients  #####\n",
    "#############################################################\n",
    "\n",
    "# Image augmentation \n",
    "transform_train = transforms.Compose([\n",
    "    # transforms.RandomCrop(32, padding=4),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    norm,\n",
    "])\n",
    "\n",
    "# if data == \"CIFAR10\":\n",
    "#     # Loading CIFAR10 using torchvision.datasets\n",
    "#     traindata = datasets.CIFAR10('./data', train=True, download=True,\n",
    "#                         transform= transform_train)\n",
    "# else:\n",
    "#     # Loading CIFAR10 using torchvision.datasets\n",
    "#     traindata = datasets.MNIST('./data', train=True, download=True,\n",
    "#                         transform= transform_train)\n",
    "\n",
    "# # Dividing the training data into num_clients, with each client having equal number of images\n",
    "# # traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
    "# traindata_split = torch.utils.data.random_split(traindata, [100 for _ in range(500)])\n",
    "# traindata_split = traindata_split[:10]\n",
    "# torch.save(traindata_split, '../../data/train_data.pth')\n",
    "\n",
    "# Creating a pytorch loader for a Deep Learning model\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in torch.load('../../data/train_data.pth')]\n",
    "\n",
    "# Normalizing the test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    norm,\n",
    "])\n",
    "\n",
    "if data == \"CIFAR10\":\n",
    "    # Loading the test iamges and thus converting them into a test_loader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            datasets.CIFAR10('./data', train=False, transform=transform_test\n",
    "            ), batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "    # Loading the test iamges and thus converting them into a test_loader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST('./data', train=False, transform=transform_test\n",
    "            ), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship \n"
     ]
    }
   ],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader[0])\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "##### Neural Network model #####\n",
    "#################################\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, channels=channels, hideen=hideen, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        act = nn.Sigmoid\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(channels, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
    "            nn.BatchNorm2d(12),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
    "            nn.BatchNorm2d(12),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=1),\n",
    "            nn.BatchNorm2d(12),\n",
    "            act(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hideen, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def client_update(client_model, optimizer, train_loader, epoch=5):\n",
    "    \"\"\"\n",
    "    This function updates/trains client model on client data\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # # show images\n",
    "            # imshow(torchvision.utils.make_grid(data))\n",
    "            # # print labels\n",
    "            # print(' '.join(f'{classes[target[j]]:5s}' for j in range(batch_size)))\n",
    "            \n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = client_model(data)\n",
    "            # loss = F.nll_loss(output, target)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward(retain_graph=True)\n",
    "            \n",
    "            received_gradients = torch.autograd.grad(loss, client_models[i].parameters())\n",
    "            received_gradients = [cg.detach()/(-0.1) for cg in received_gradients]\n",
    "            \n",
    "            cpl_attacker = GradientInversion_Attack(client_models[i], img_shape, lr=10e2, log_interval=0,\n",
    "                            optimizer_class=torch.optim.SGD,\n",
    "                            distancename=\"l2\", optimize_label=False,\n",
    "                            num_iteration=10000,\n",
    "                            lm_reg_coef=0.01, device=device)\n",
    "            \n",
    "            result = cpl_attacker.attack(received_gradients)\n",
    "            \n",
    "            num_seeds=3\n",
    "            fig = plt.figure()\n",
    "            for s in tqdm(range(num_seeds)):\n",
    "                ax1 = fig.add_subplot(3, num_seeds, s+1)\n",
    "                test_img = torch.from_numpy((result[0].cpu().detach().numpy()[0]))\n",
    "                img1 = test_img.swapaxes(0,1)\n",
    "                img1 = img1.swapaxes(1,2)\n",
    "                ax1.imshow(torchvision.utils.make_grid(img1)\n",
    "                        )\n",
    "                ax1.set_title(torch.argmax(result[1]).item())\n",
    "                ax1.axis(\"off\")\n",
    "                ax2 = fig.add_subplot(3, num_seeds, num_seeds+s+1)\n",
    "                ax2.imshow(result[0].cpu().detach().numpy()[0][0], cmap=\"gray\")\n",
    "                ax2.axis(\"off\")\n",
    "                \n",
    "            plt.suptitle(\"Result of CPL\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "                \n",
    "            optimizer.step()\n",
    "    return loss, received_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_aggregate(global_model, client_models):\n",
    "    \"\"\"\n",
    "    This function has aggregation method 'mean'\n",
    "    \"\"\"\n",
    "    ### This will take simple mean of the weights of models ###\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(global_model, test_loader):\n",
    "    \"\"\"This function test the global model on test data and returns test loss and test accuracy \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = global_model(data)\n",
    "            # test_loss += F.nll_loss(output, target).item() # sum up batch loss\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = correct / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#### Initializing models and optimizer  ####\n",
    "############################################\n",
    "\n",
    "#### global model ##########\n",
    "global_model =  VGG().cuda()\n",
    "\n",
    "############## client models ##############\n",
    "client_models = [ VGG().cuda() for _ in range(num_selected)]\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict()) ### initial synchronizing with global model \n",
    "\n",
    "############### optimizers ################\n",
    "opt = [optim.SGD(model.parameters(), lr=0.1) for model in client_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "###### List containing info about learning #########\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "# Runnining FL\n",
    "\n",
    "victim_count = 0 \n",
    "for r in range(num_rounds):\n",
    "    # select random clients\n",
    "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
    "    # client update\n",
    "    loss = 0\n",
    "    \n",
    "    for i in tqdm(range(num_selected)):\n",
    "        client_loss, received_gradients = client_update(client_models[i], opt[i], train_loader[client_idx[i]], epoch=1)\n",
    "        loss += client_loss.item()\n",
    "        # if client_idx[i] == 3:\n",
    "        # cpl_attacker = GradientInversion_Attack(client_models[i], img_shape, lr=10e2, log_interval=0,\n",
    "        #                     optimizer_class=torch.optim.SGD,\n",
    "        #                     distancename=\"l2\", optimize_label=False,\n",
    "        #                     num_iteration=1000,\n",
    "        #                     lm_reg_coef=0.01, device=device)\n",
    "        # result = cpl_attacker.attack(received_gradients)\n",
    "        \n",
    "        # num_seeds=5\n",
    "        # fig = plt.figure()\n",
    "        # for s in tqdm(range(num_seeds)):\n",
    "        #     ax1 = fig.add_subplot(3, num_seeds, s+1)\n",
    "        #     test_img = torch.from_numpy((result[0].cpu().detach().numpy()[0]))\n",
    "        #     img1 = test_img.swapaxes(0,1)\n",
    "        #     img1 = img1.swapaxes(1,2)\n",
    "        #     ax1.imshow(torchvision.utils.make_grid(img1)\n",
    "        #                )\n",
    "        #     ax1.set_title(torch.argmax(result[1]).item())\n",
    "        #     ax1.axis(\"off\")\n",
    "        #     ax2 = fig.add_subplot(3, num_seeds, num_seeds+s+1)\n",
    "        #     ax2.imshow(result[0].cpu().detach().numpy()[0][0], cmap=\"gray\")\n",
    "        #     ax2.axis(\"off\")\n",
    "            \n",
    "        # plt.suptitle(\"Result of CPL\")\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "            \n",
    "        # gradinversion = GradientInversion_Attack(client_models[i], img_shape, num_iteration=500,\n",
    "        #                         lr=1e2, log_interval=0,\n",
    "        #                         optimizer_class=torch.optim.SGD,\n",
    "        #                         distancename=\"l2\", optimize_label=False,\n",
    "        #                         bn_reg_layers=[client_models[i].body[1], client_models[i].body[4], client_models[i].body[7]],\n",
    "        #                         group_num = 5,\n",
    "        #                         tv_reg_coef=0.00, l2_reg_coef=0.0001,\n",
    "        #                         bn_reg_coef=0.001, gc_reg_coef=0.001, device=device)\n",
    "        \n",
    "        # result = gradinversion.group_attack(received_gradients, batch_size=batch_size)\n",
    "\n",
    "        # fig = plt.figure()\n",
    "        # for bid in range(batch_size):\n",
    "        #     ax1 = fig.add_subplot(2, batch_size, bid+1)\n",
    "        #     test_img = torch.from_numpy(((sum(result[0]) / len(result[0])).cpu().detach().numpy()[bid]))\n",
    "        #     img1 = test_img.swapaxes(0,1)\n",
    "        #     img1 = img1.swapaxes(1,2)\n",
    "        #     ax1.imshow(torchvision.utils.make_grid(img1))\n",
    "        #     ax1.set_title(result[1][0][bid].item())\n",
    "        #     ax1.axis(\"off\")\n",
    "        #     ax2 = fig.add_subplot(2, batch_size, batch_size+bid+1)\n",
    "        #     ax2.imshow((sum(result[0]) / len(result[0])).cpu().detach().numpy()[bid][0], cmap=\"gray\")\n",
    "        #     ax2.axis(\"off\")\n",
    "\n",
    "        # plt.suptitle(\"Result of GradInversion\")\n",
    "    \n",
    "    losses_train.append(loss)\n",
    "    # server aggregate\n",
    "    server_aggregate(global_model, client_models)\n",
    "    \n",
    "    test_loss, acc = test(global_model, test_loader)\n",
    "    losses_test.append(test_loss)\n",
    "    acc_test.append(acc)\n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, acc))\n",
    "print(torch.nn.parameter.Parameter( global_model.fc.weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "968934d88acb7fa4003ed11a52074e68ba95c397f9757dc45b04b92654f10ea7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
